{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Assets/header.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Contents`\n",
    "\n",
    "- [Strategies to obtain the data](#strat)\n",
    "- [Load Libraries](#load)   \n",
    "- [Facebook Scraping Script](#face)  \n",
    "- [Save raw CSV's](#csv)  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"strat\"></a>\n",
    "# `Strategies to obtain the data`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have identified three strategies to obtain Facebook page data, all with varying degrees of acquisition complexity and data quality: \n",
    "\n",
    "    -Use the Graph API through Facebook (Gold Medal)\n",
    "    -Use some web scraping/parsing script (Silver Medal)\n",
    "    -Using a pre-existing data set from Kaggle or another data set repository (Bronze Medal) \n",
    "\n",
    "The ideal / 'Gold Medal' approach of obtaining Facebook page data is through the Facebook Graph API. However, ever since the [Cambridge Analytica debacle](\"https://medium.com/tow-center/the-graph-api-key-points-in-the-facebook-and-cambridge-analytica-debacle-b69fe692d747\"), Facebook have severely limited developers’ access to page data for brands. \n",
    "\n",
    "\n",
    "<img src=\"http://www.speaklikeapro.co.uk/Images/bouncer%20b&w.jpg\" style=\"width: 200px;\">\n",
    "\n",
    "\n",
    "\n",
    "With option one out of the window, I explored online for pre-existing data sets that had the data I needed. Unfortunately I couldn't find anything that met my needs so I decided to proceed with strategy 2 and use Selenium to write a script that would in effect ‘navigate’ Facebook like a user, scraping the data from each post on any given brands Facebook page. So sneaky.\n",
    "\n",
    "## Using Selenium\n",
    "\n",
    "Having experimented with Facebook.com (desktop site), I found out quite quickly that I got logged out several times with multiple log-in pages and more complex html to naviagate. In all, the dekstop site is quite difficult to manipulate. Fortunately ‘mobile.facebook.com’ proved to be far easier to manipulate. *However* after exploring mobile.Facebook.com, I found that many brands only hosted a year's worth of data - about 350 records on average which wouldn't be enough to feed into a classifier. I finally decided to use the desktop site where the content usually goes back many years and design a more comprehensive script that will capture the data I need.\n",
    "\n",
    "My main aim was to obtain the following data from each post:\n",
    "\n",
    "    -Post content (i.e. the copy of the post)\n",
    "    -The date post was made\n",
    "    -The number of comments \n",
    "    -The number of responses overall\n",
    "    -The number of responses broken down by emotion (likes, love, wow, angry, sad, haha)\n",
    "    -The number of shares\n",
    "    -The type of post i.e. (status update, photo, video, poll)\n",
    "\n",
    "\n",
    "## Additional objective:\n",
    "\n",
    "Can a machine learning classification model learn to differentiate between the social content of the main UK supermarket brands?\n",
    "\n",
    "The commercial implications of this analysis are interesting. If the experimental hypothesis is verified i.e. there is a genuine difference in the social content between all the supermarket brands in the UK (which a classifier will be able to identify with a high degree of accuracy) then the various brand managers and marketing teams are doing a good job! However if the null hypothesis is confirmed – and there is no observable difference in the social content of all the main brands then this is problematic. A key aspect of being a healthy brand is being a differentiated brand. It will also be interesting to explore the distribution of precision and recall - are more premium/economy brands more likely to be misclassified as other premium/economy brands?\n",
    "\n",
    "## Data to obtain:\n",
    "\n",
    "I’ve chosen a range of UK super markets; a mixture of higher end and lower end. I’ve also chosen brands that are fairly active on Facebook and have a reasonable number of engaged users. The following brands have been scraped:\n",
    "\n",
    "    -Sainsburys\n",
    "    -Tesco \n",
    "    -Waitrose\n",
    "    -M&S\n",
    "    -Morrisons\n",
    "    -Lidl\n",
    "    -ASDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "# `Load Libraries`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import ast\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"face\"></a>\n",
    "# `Facebook Scraping Script`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in details and brand we want to scrape (it doesn't even have to be a brand - it can be any entity that owns a commerical Facebook page - try it!)\n",
    "user = '==PUT YOUR LOG IN HERE=='\n",
    "password = '==PUT YOUR PASSWORD HERE=='\n",
    "brand = \"==PUT FACEBOOK BRAND URL HERE e.g.'Tesco'==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped : 1\n",
      "Scraped : 2\n",
      "Scraped : 3\n",
      "Scraped : 4\n",
      "Scraped : 5\n",
      "Scraped : 6\n",
      "Scraped : 7\n",
      "Scraped : 8\n",
      "Scraped : 9\n",
      "Scraped : 10\n",
      "Scraped : 11\n",
      "Scraped : 12\n",
      "Scraped : 13\n",
      "Scraped : 14\n",
      "Scraped : 15\n",
      "Scraped : 16\n",
      "Scraped : 17\n",
      "Scraped : 18\n",
      "Scraped : 19\n",
      "Scraped : 20\n",
      "Scraped : 21\n",
      "Scraped : 22\n",
      "Scraped : 23\n",
      "Scraped : 24\n",
      "Scraped : 25\n",
      "Scraped : 26\n",
      "Scraped : 27\n",
      "Scraped : 28\n",
      "Scraped : 29\n",
      "Scraped : 30\n",
      "Scraped : 31\n",
      "Scraped : 32\n",
      "Scraped : 33\n",
      "Scraped : 34\n",
      "Scraped : 35\n",
      "Scraped : 36\n",
      "Scraped : 37\n",
      "Scraped : 38\n",
      "Scraped : 39\n",
      "Scraped : 40\n",
      "Scraped : 41\n",
      "Scraped : 42\n",
      "Scraped : 43\n",
      "Scraped : 44\n",
      "Scraped : 45\n",
      "Scraped : 46\n",
      "Scraped : 47\n",
      "Scraped : 48\n",
      "Scraped : 49\n",
      "Scraped : 50\n",
      "Scraped : 51\n",
      "Scraped : 52\n",
      "Scraped : 53\n",
      "Scraped : 54\n",
      "Scraped : 55\n",
      "Scraped : 56\n",
      "Scraped : 57\n",
      "Scraped : 58\n",
      "Facebook scrape of LidlUK scraped 58 Facebook posts.\n"
     ]
    }
   ],
   "source": [
    "#initialise web driver\n",
    "driver = webdriver.Chrome('./Libraries/chromedriver')\n",
    "\n",
    "#FACEBOOK LOG IN \n",
    "driver.get('https://www.facebook.com/')\n",
    "email = driver.find_element_by_xpath('//*[@id=\"email\"]')\n",
    "email.send_keys(user)\n",
    "\n",
    "pass_1 = driver.find_element_by_xpath('//*[@id=\"pass\"]')\n",
    "pass_1.send_keys(password)\n",
    "\n",
    "click_1 = driver.find_element_by_xpath('//*[@id=\"loginbutton\"]')\n",
    "click_1.click()\n",
    "\n",
    "#GO TO BRAND PAGE    \n",
    "driver.get(f'https://www.facebook.com/{brand}')  \n",
    "           \n",
    "#SCRAPING STARTS HERE===========================\n",
    "\n",
    "start = time.time()\n",
    "while time.time()-start<5:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "#list of each posts text/content\n",
    "post_text = []\n",
    "#list of each posts comments and share count\n",
    "post_comments_shares = []\n",
    "#list of dates when each post was made\n",
    "post_date = []\n",
    "#list of total interactions(e.g. all likes, love, haha, wow, sad, angry)\n",
    "post_all_response = []\n",
    "#row labels for brand scraped\n",
    "brand_list = []\n",
    "\n",
    "#THIS IS WHERE WE INITATE THE SCRAPE  \n",
    "item = driver.find_elements_by_class_name('_4-u2')\n",
    "\n",
    "#scraping all the content we want and appending to the lists made above\n",
    "for each in item:\n",
    "    try:\n",
    "        post_text.append(each.find_element_by_class_name('_5pbx').text)\n",
    "    except:\n",
    "        post_text.append(np.nan)\n",
    "    try:\n",
    "        post_date.append(each.find_element_by_class_name('timestampContent').text)\n",
    "    except:\n",
    "        post_date.append(np.nan)\n",
    "    try:\n",
    "        post_comments_shares.append(each.find_element_by_class_name('_ipo').text)\n",
    "    except:\n",
    "        post_comments_shares.append(np.nan)\n",
    "    try:\n",
    "        post_all_response.append(each.find_element_by_class_name('_4arz').text)\n",
    "    except:\n",
    "        post_all_response.append(np.nan)\n",
    "    print(\"Scraped :\",len(post_text))\n",
    "    \n",
    "#Setting up a list of brand labels\n",
    "count = len(post_text)\n",
    "while count > 0:\n",
    "    brand_list.append(brand)\n",
    "    count -= 1\n",
    "\n",
    "#Set Up Data frame for raw data\n",
    "items = pd.DataFrame({'Post_Content': post_text,\n",
    "                      #'Type': post_type,\n",
    "                      'Date': post_date,\n",
    "                      'Comments_Shares': post_comments_shares,\n",
    "                      'All_Responses': post_all_response,\n",
    "                      'Brand' : brand_list\n",
    "                     })\n",
    "\n",
    "print(f\"Facebook scrape of {brand} scraped {len(post_text)} Facebook posts.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"csv\"></a>\n",
    "# `Save the raw CSV file`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.to_csv(f\"{brand}final_desktop.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've now scraped all the Facebook content for the following UK supermarkets/retailers:\n",
    "\n",
    "    - Tesco\n",
    "    - Sainsbury's\n",
    "    - ASDA\n",
    "    - Waitrose\n",
    "    - Lidl\n",
    "    - M&S\n",
    "    - Morrisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Next Steps:`\n",
    "---\n",
    "\n",
    "The next step is to import all of them, concat them and then do some fairly heavy duty cleaning on them - they're looking a little messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comments_Shares</th>\n",
       "      <th>All_Responses</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fuel your workout regime with this selection o...</td>\n",
       "      <td>2 hrs</td>\n",
       "      <td>66 Comments19 Shares</td>\n",
       "      <td>41</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fuel your workout regime with this selection o...</td>\n",
       "      <td>2 hrs</td>\n",
       "      <td>66 Comments19 Shares</td>\n",
       "      <td>41</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Add a fountain of style to your home with this...</td>\n",
       "      <td>January 27 at 1:00 PM</td>\n",
       "      <td>83 Comments29 Shares</td>\n",
       "      <td>254</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We've teamed up with our friends at Proper Tas...</td>\n",
       "      <td>January 26 at 3:30 PM</td>\n",
       "      <td>51 Comments156 Shares78K Views</td>\n",
       "      <td>463</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>We've teamed up with our friends at Proper Tas...</td>\n",
       "      <td>January 26 at 3:30 PM</td>\n",
       "      <td>51 Comments156 Shares78K Views</td>\n",
       "      <td>463</td>\n",
       "      <td>LidlUK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Post_Content                   Date  \\\n",
       "0                                                 NaN                    NaN   \n",
       "1                                                 NaN                    NaN   \n",
       "2                                                 NaN                    NaN   \n",
       "3                                                 NaN                    NaN   \n",
       "4                                                 NaN                    NaN   \n",
       "5                                                 NaN                    NaN   \n",
       "6                                                 NaN                    NaN   \n",
       "7                                                 NaN                    NaN   \n",
       "8   Fuel your workout regime with this selection o...                  2 hrs   \n",
       "9   Fuel your workout regime with this selection o...                  2 hrs   \n",
       "10  Add a fountain of style to your home with this...  January 27 at 1:00 PM   \n",
       "11                                                NaN                    NaN   \n",
       "12                                                NaN                    NaN   \n",
       "13  We've teamed up with our friends at Proper Tas...  January 26 at 3:30 PM   \n",
       "14  We've teamed up with our friends at Proper Tas...  January 26 at 3:30 PM   \n",
       "\n",
       "                   Comments_Shares All_Responses   Brand  \n",
       "0                              NaN           NaN  LidlUK  \n",
       "1                              NaN           NaN  LidlUK  \n",
       "2                              NaN           NaN  LidlUK  \n",
       "3                              NaN           NaN  LidlUK  \n",
       "4                              NaN           NaN  LidlUK  \n",
       "5                              NaN           NaN  LidlUK  \n",
       "6                              NaN           NaN  LidlUK  \n",
       "7                              NaN           NaN  LidlUK  \n",
       "8             66 Comments19 Shares            41  LidlUK  \n",
       "9             66 Comments19 Shares            41  LidlUK  \n",
       "10            83 Comments29 Shares           254  LidlUK  \n",
       "11                             NaN           NaN  LidlUK  \n",
       "12                             NaN           NaN  LidlUK  \n",
       "13  51 Comments156 Shares78K Views           463  LidlUK  \n",
       "14  51 Comments156 Shares78K Views           463  LidlUK  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
